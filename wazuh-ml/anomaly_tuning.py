#!/usr/bin/env python3
# anomaly_tuning.py
"""
Module tinh ch·ªânh anomaly detection ƒë·ªÉ gi·∫£m false positives
"""

import pandas as pd
import numpy as np
from datetime import datetime

class AnomalyFilter:
    """Filter v√† ƒëi·ªÅu ch·ªânh anomaly scores ƒë·ªÉ gi·∫£m false positives"""
    
    def __init__(self):
        # Whitelist: C√°c s·ª± ki·ªán ƒë∆∞·ª£c coi l√† b√¨nh th∆∞·ªùng
        self.whitelisted_patterns = {
            'ssh_internal_admin': {
                'description': 'SSH login t·ª´ admin n·ªôi b·ªô',
                'conditions': {
                    'event_desc': ['sshd: authentication success'],
                    'src_ip': ['172.16.158.1', '172.16.158.100'],  # Admin IPs
                    'is_business_hours': [1],  # Ch·ªâ trong gi·ªù l√†m vi·ªác
                }
            },
            'system_update': {
                'description': 'System updates v√† package management',
                'conditions': {
                    'event_desc': ['apt user-agent', 'package management'],
                    'rule_level': [0, 1, 2, 3, 4]  # Low severity
                }
            },
            'scheduled_integrity_check': {
                'description': 'FIM checks ƒë·ªãnh k·ª≥',
                'conditions': {
                    'event_desc': ['integrity checksum changed'],
                    'hour': [2, 3],  # Scheduled at 2-3 AM
                    'rule_level': [0, 1, 2, 3, 4, 5, 6, 7]
                }
            },
            'compliance_check': {
                'description': 'CIS compliance checks',
                'conditions': {
                    'event_desc': ['cis', 'benchmark', 'status changed from failed to passed']
                }
            }
        }
        
        # Suspicious patterns (tƒÉng score)
        self.suspicious_patterns = {
            'brute_force': {
                'description': 'Brute force attempts',
                'conditions': {
                    'event_desc': ['non-existent user', 'failed password', 'invalid user'],
                },
                'score_multiplier': 2.0
            },
            'night_activity': {
                'description': 'Activity v√†o ban ƒë√™m (ngo·∫°i tr·ª´ scheduled tasks)',
                'conditions': {
                    'is_night': [1],
                    'rule_level': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                },
                'score_multiplier': 1.5
            },
            'external_access': {
                'description': 'Access t·ª´ external IPs',
                'conditions': {
                    'is_internal_src': [0],  # External source
                    'rule_level': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                },
                'score_multiplier': 1.8
            },
            'high_severity': {
                'description': 'High/Critical severity events',
                'conditions': {
                    'is_critical': [1],
                },
                'score_multiplier': 2.5
            },
            'burst_activity': {
                'description': 'Burst of events',
                'conditions': {
                    'is_burst': [1],
                },
                'score_multiplier': 1.3
            }
        }
    
    def match_pattern(self, row, conditions):
        """
        Ki·ªÉm tra xem row c√≥ match v·ªõi pattern conditions kh√¥ng
        
        Args:
            row: DataFrame row
            conditions: Dictionary v·ªõi pattern conditions
        
        Returns:
            True n·∫øu match, False n·∫øu kh√¥ng
        """
        for col, values in conditions.items():
            if col not in row.index:
                continue
            
            row_value = row[col]
            
            # String matching (contains)
            if isinstance(values[0], str):
                if not any(str(v).lower() in str(row_value).lower() for v in values):
                    return False
            # Exact matching
            else:
                if row_value not in values:
                    return False
        
        return True
    
    def apply_whitelist(self, df):
        """
        √Åp d·ª•ng whitelist ƒë·ªÉ lo·∫°i b·ªè false positives
        
        Args:
            df: DataFrame v·ªõi anomaly predictions
        
        Returns:
            DataFrame v·ªõi whitelist flags
        """
        df = df.copy()
        df['is_whitelisted'] = False
        df['whitelist_reason'] = ''
        
        for pattern_name, pattern_config in self.whitelisted_patterns.items():
            conditions = pattern_config['conditions']
            description = pattern_config['description']
            
            for idx, row in df.iterrows():
                if self.match_pattern(row, conditions):
                    df.at[idx, 'is_whitelisted'] = True
                    df.at[idx, 'whitelist_reason'] = description
        
        return df
    
    def apply_suspicious_boost(self, df):
        """
        TƒÉng anomaly score cho c√°c pattern ƒë√°ng ng·ªù
        
        Args:
            df: DataFrame v·ªõi anomaly scores
        
        Returns:
            DataFrame v·ªõi adjusted scores
        """
        df = df.copy()
        df['score_multiplier'] = 1.0
        df['suspicious_reason'] = ''
        
        for pattern_name, pattern_config in self.suspicious_patterns.items():
            conditions = pattern_config['conditions']
            multiplier = pattern_config.get('score_multiplier', 1.0)
            description = pattern_config['description']
            
            for idx, row in df.iterrows():
                if self.match_pattern(row, conditions):
                    # √Åp d·ª•ng multiplier cao nh·∫•t n·∫øu match nhi·ªÅu patterns
                    if multiplier > df.at[idx, 'score_multiplier']:
                        df.at[idx, 'score_multiplier'] = multiplier
                        df.at[idx, 'suspicious_reason'] = description
        
        # Adjust anomaly score
        df['anomaly_score_original'] = df['anomaly_score']
        df['anomaly_score'] = df['anomaly_score'] * df['score_multiplier']
        
        return df
    
    def filter_anomalies(self, df, remove_whitelisted=True, apply_boost=True):
        """
        √Åp d·ª•ng to√†n b·ªô filters
        
        Args:
            df: DataFrame v·ªõi anomaly predictions
            remove_whitelisted: Lo·∫°i b·ªè whitelisted events kh·ªèi anomalies
            apply_boost: TƒÉng score cho suspicious events
        
        Returns:
            Filtered DataFrame
        """
        df = df.copy()
        
        # Apply whitelist
        df = self.apply_whitelist(df)
        
        # Apply suspicious boost
        if apply_boost:
            df = self.apply_suspicious_boost(df)
        
        # Filter anomalies
        if remove_whitelisted:
            # Ch·ªâ gi·ªØ anomalies kh√¥ng b·ªã whitelist
            df['anomaly_label_filtered'] = df.apply(
                lambda row: -1 if (row['anomaly_label'] == -1 and not row['is_whitelisted']) else 1,
                axis=1
            )
        else:
            df['anomaly_label_filtered'] = df['anomaly_label']
        
        return df


def analyze_anomaly_distribution(df):
    """
    Ph√¢n t√≠ch ph√¢n b·ªë c·ªßa anomalies
    
    Args:
        df: DataFrame v·ªõi anomaly predictions
    """
    print("\n" + "="*70)
    print("üìä ANOMALY DISTRIBUTION ANALYSIS")
    print("="*70)
    
    anomalies = df[df['anomaly_label'] == -1]
    
    if len(anomalies) == 0:
        print("No anomalies detected!")
        return
    
    # By severity
    print("\nüî¥ By Severity:")
    if 'severity_category' in df.columns:
        severity_dist = anomalies['severity_category'].value_counts()
        for severity, count in severity_dist.items():
            print(f"  {severity:10s}: {count:3d} ({count/len(anomalies)*100:.1f}%)")
    
    # By time of day
    print("\n‚è∞ By Time:")
    if 'hour' in df.columns:
        hour_dist = anomalies['hour'].value_counts().sort_index()
        for hour, count in hour_dist.items():
            bar = "‚ñà" * int(count / len(anomalies) * 50)
            print(f"  {int(hour):02d}:00 - {count:3d} {bar}")
    
    # By agent
    print("\nüíª By Agent:")
    if 'agent' in df.columns:
        agent_dist = anomalies['agent'].value_counts()
        for agent, count in agent_dist.head(5).items():
            print(f"  {agent:20s}: {count:3d} ({count/len(anomalies)*100:.1f}%)")
    
    # By event type
    print("\nüìù Top Event Types:")
    if 'event_desc' in df.columns:
        event_dist = anomalies['event_desc'].value_counts()
        for event, count in event_dist.head(10).items():
            print(f"  {event[:60]:60s}: {count:3d}")
    
    # Score distribution
    print("\nüìà Score Distribution:")
    if 'anomaly_score' in df.columns:
        scores = anomalies['anomaly_score']
        print(f"  Min:    {scores.min():.4f}")
        print(f"  Q1:     {scores.quantile(0.25):.4f}")
        print(f"  Median: {scores.median():.4f}")
        print(f"  Q3:     {scores.quantile(0.75):.4f}")
        print(f"  Max:    {scores.max():.4f}")


def recommend_threshold(df, target_anomaly_rate=0.02):
    """
    ƒê·ªÅ xu·∫•t threshold d·ª±a tr√™n ph√¢n b·ªë scores
    
    Args:
        df: DataFrame v·ªõi anomaly scores
        target_anomaly_rate: T·ª∑ l·ªá anomaly mong mu·ªën (default 2%)
    
    Returns:
        Recommended threshold
    """
    scores = df['anomaly_score'].values
    scores_sorted = np.sort(scores)
    
    # T√≠nh threshold t·∫°i target percentile
    threshold_idx = int(len(scores) * target_anomaly_rate)
    recommended_threshold = scores_sorted[threshold_idx]
    
    print("\n" + "="*70)
    print("üí° THRESHOLD RECOMMENDATION")
    print("="*70)
    print(f"Target anomaly rate: {target_anomaly_rate*100:.1f}%")
    print(f"Recommended threshold: {recommended_threshold:.4f}")
    print(f"Expected anomalies: {threshold_idx}")
    
    # Test v·ªõi c√°c threshold kh√°c nhau
    print("\nüìä Threshold Impact:")
    for percentile in [0.01, 0.02, 0.03, 0.05, 0.07, 0.10]:
        threshold = scores_sorted[int(len(scores) * percentile)]
        n_anomalies = (scores <= threshold).sum()
        print(f"  {percentile*100:4.1f}% ‚Üí threshold={threshold:7.4f} ‚Üí {n_anomalies:3d} anomalies")
    
    return recommended_threshold


if __name__ == "__main__":
    # Test v·ªõi data th·ª±c
    print("Testing Anomaly Tuning Module...")
    
    from config import ANALYZED_CSV_PATH
    
    if not pd.io.common.file_exists(ANALYZED_CSV_PATH):
        print("‚ùå No analyzed data found. Run train_model.py first.")
        exit(1)
    
    df = pd.read_csv(ANALYZED_CSV_PATH)
    print(f"Loaded {len(df)} records")
    
    # Analyze distribution
    analyze_anomaly_distribution(df)
    
    # Recommend threshold
    recommend_threshold(df, target_anomaly_rate=0.02)
    
    # Apply filters
    print("\n" + "="*70)
    print("üîß APPLYING FILTERS")
    print("="*70)
    
    filter_engine = AnomalyFilter()
    df_filtered = filter_engine.filter_anomalies(df)
    
    before = (df['anomaly_label'] == -1).sum()
    after = (df_filtered['anomaly_label_filtered'] == -1).sum()
    whitelisted = df_filtered['is_whitelisted'].sum()
    
    print(f"\nüìä Filtering Results:")
    print(f"  Before filtering: {before} anomalies")
    print(f"  Whitelisted:      {whitelisted} events")
    print(f"  After filtering:  {after} anomalies")
    print(f"  Reduction:        {before - after} ({(before-after)/before*100:.1f}%)")
    
    print("\n‚úÖ Filtered anomalies:")
    filtered_anomalies = df_filtered[df_filtered['anomaly_label_filtered'] == -1]
    if len(filtered_anomalies) > 0:
        display_cols = ['timestamp', 'agent', 'rule_level', 'event_desc', 'anomaly_score', 'suspicious_reason']
        display_cols = [c for c in display_cols if c in filtered_anomalies.columns]
        print(filtered_anomalies[display_cols].head(10).to_string(index=False))
