import os
import requests
import json
import pandas as pd
import urllib3
from config import (
    WAZUH_INDEXER_URL,
    WAZUH_INDEX_PATTERN,
    INDEXER_USER,
    INDEXER_PASS,
    RAW_JSON_PATH,
    CSV_PATH,
    VERIFY_SSL,
)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def fetch_logs():
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c data/ t·ªìn t·∫°i
    os.makedirs(os.path.dirname(RAW_JSON_PATH), exist_ok=True)

    # Query ƒë√£ m·ªü r·ªông tr∆∞·ªùng
    query = {
        "size": 10000,
        "_source": [
            "@timestamp",
            "agent.name",
            "integration",
            "rule.id",
            "rule.level",
            "rule.groups",
            "rule.category",
            "rule.description",
            "data.proto",
            "data.srcip",
            "data.srcport",
            "data.dstip",
            "data.dstport",
            "data.bytes",
            "data.len"
        ],
        "query": { "match_all": {} }
    }

    url = f"{WAZUH_INDEXER_URL}/{WAZUH_INDEX_PATTERN}/_search"
    print(f"üì° ƒêang truy v·∫•n d·ªØ li·ªáu t·ª´ {url} ...")

    resp = requests.post(
        url,
        auth=(INDEXER_USER, INDEXER_PASS),
        json=query,
        verify=VERIFY_SSL
    )
    resp.raise_for_status()
    data = resp.json()

    # L∆∞u b·∫£n raw JSON ƒë·ªÉ tham chi·∫øu / debug
    with open(RAW_JSON_PATH, "w") as f:
        json.dump(data, f, indent=2)
    print(f"‚úÖ ƒê√£ l∆∞u file JSON th√¥ ‚Üí {RAW_JSON_PATH}")

    # Parse hits -> rows
    hits = data.get("hits", {}).get("hits", [])
    rows = []
    for h in hits:
        src = h.get("_source", {})
        rule = src.get("rule", {}) or {}
        dat = src.get("data", {}) or {}
        agent = src.get("agent", {}) or {}

        rows.append({
            "timestamp": src.get("@timestamp"),
            "agent": agent.get("name"),
            "integration": src.get("integration"),
            "rule_id": rule.get("id"),
            "rule_level": rule.get("level"),
            "rule_groups": rule.get("groups"),      # c√≥ th·ªÉ l√† list
            "rule_category": rule.get("category"),
            "event_desc": rule.get("description"),
            "proto": dat.get("proto"),
            "src_ip": dat.get("srcip"),
            "src_port": dat.get("srcport"),
            "dst_ip": dat.get("dstip"),
            "dst_port": dat.get("dstport"),
            "bytes": dat.get("bytes"),
            "length": dat.get("len")
        })

    df = pd.DataFrame(rows)

    # üîÅ CH·∫∂N L·ªñI ·ªû ƒê√ÇY:
    # B·∫•t k·ª≥ √¥ n√†o l√† list (v√≠ d·ª• ["sshd","authentication_success"]) -> convert sang chu·ªói "sshd,authentication_success"
    def normalize_cell(x):
        if isinstance(x, list):
            return ", ".join([str(i) for i in x])
        return x

    df = df.applymap(normalize_cell)

    # B√¢y gi·ªù m·ªçi √¥ ƒë·ªÅu hashable -> c√≥ th·ªÉ drop_duplicates an to√†n
    df = df.drop_duplicates()

    # S·∫Øp x·∫øp theo th·ªùi gian (n·∫øu timestamp c√≥ None, pandas v·∫´n ch·ªãu ƒë∆∞·ª£c sort_values v·ªõi na_position)
    df = df.sort_values(by="timestamp", na_position="last", ignore_index=True)

    # Ghi CSV cu·ªëi c√πng
    df.to_csv(CSV_PATH, index=False)
    print(f"‚úÖ ƒê√£ l∆∞u {len(df)} d√≤ng log ‚Üí {CSV_PATH}")

if __name__ == "__main__":
    fetch_logs()
